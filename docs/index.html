<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="style.css">




</head>
<body>

<div class="header-image">
    <div class="header-text">
        <h1>Stop Sign Detection</h1>
        <h4>Han Cao, Xi Chen, Lichengxi Huang</h4>
    </div>
</div>

<div class="main">
    <h2>Background</h2>

    <p> Traffic signs play a significant role in the modern traffic system, and it becomes even more important in recent years due to the emergence of autonomous vehicles. The problem we are aiming to solve is to use the computer vision technologies to do the stop sign detection under different driving scenarios and extreme cases.
    </p>

    <br>

    <h2>Motivation</h2>
    
    <p>With the development of autonomous vehicles, recognizing stop signs plays a very important role. Although in the future, when connected autonomous vehicles are developed, this may be solved by using roadside units sending instructions to the vehicles, nowadays, we still need to rely on the vehicles themselves to recognize these signs. Furthermore, vehicles that can recognize traffic signs will improve the driving experiences and safety for the drivers. Most drivers may have experienced situations when they were driving too fast on the highways and did not see clearly what the road guidance sign tells, hence driving to the wrong direction. In other cases, drivers may not see speed limit signs when the environment is dark, hence getting a ticket on over speeding.</p>
    
    <p>These scenarios can all be avoided as long as we can develop an application that can automatically recognize the traffic signs. The vehicles that are equipped with this application can extract the key information, and either speak it out or display it on the head-up displays (HUD).
    </p>

    <p>For our project, we focus on stop sign detection.</p>

    <br>
    
    <h2>Current state-of-the-art</h2>
    <p> In general speaking of stop signs detection technology, itâ€™s already well developed. It is a significant part of Advanced driver-assistance systems(ADAS). Such technologies are already used broadly by vehicle manufacturers on their products, such as BMW, Ford and Volvo, to improve the performance and safety. Most cars with an ADAS are able to detect stop signs.</p>

    <p>The cornerstones of the stop signs detection are based on two facts:</p>

    <ol>
        <li>The features of stop signs are identical in the U.S. There is a standard of the design and size of all traffic sign published by MUTCD in the U.S.</li>
        <li>The cutting-edge algorithms enable cars to analyze and understand the content of stop signs.</li>
    </ol>

    <p>The concept was brought up in 1968, but the recent blossom was triggered by the breaking through in hardware and machine learning algorithms.</p>
    
    <p>Following are some widely-used approaches to the problem:</p>
    <ul>
        <li>Histogram of oriented gradients (HOG) + supervised learning (SVM, random forest)</li>
        <li>Deep learning approaches (RCNN, Mask-RCNN, Fast-RCNN, YOLO, ...)</li>
    </ul>
    
    <br>

    <h2>Structure of the webpage</h2>
    <p>In the following of the webpage, we will talk about the two approaches we implemented: the deep learning approach and the traditional computer vision approach associated with the RBF kernel SVM classifier.</p>

    <br>

    <h2>Approach 1 - deep learning</h2>
    <h3>Overview</h3>
    <p>In this part, we tried to use YOLOv3 (You Only Look Once v3) to perform traffic sign detection with COCO dataset and LISA Traffic Sign Dataset (LISA-TS), we examed our results by using challenging condition pictures.</p>

    <h3>First try: YOLOv3 + COCO</h3>
    <h4>COCO data set</h4>
    <p>COCO is a large-scale object detection, segmentation, and captioning dataset. We chose COCO as our data set because we do not need to reformat the annoation, and COCO has a large set of training data of stop sign.</p>
    
    <p>The resolution of the training data is high and all pictures were taken under normal conditions.</p>

    <p>A peek of COCO data set:</p>

    <img class="center" src="COCO_PEEK.png" alt="COCO_PEEK" width="700">

    <h4>Input data preparation</h4>
    <p>We used COCO API to preprocess our data using the following command:</p>
    <code style="font-size:12pt"># get all images containing given categories, select one at random<br>
        catIds = coco.getCatIds(catNms=['stop_sign']);<br>
        imgIds = coco.getImgIds(catIds=catIds );
    </code>

    
    <h4>YOLOv3</h4>
    <p>YOLOv3 is a very famous object detector, which has the following advantages:</p>
    <ul>
        <li>Fast: Can perform detection in 30FPS live video</li>
        <li>Arcurate: Have a fairly good arcuracy in pre-trained model</li>
        <li>Active community: Easy to find solution</li>
    </ul>

    <h4>Training YOLOv3</h4>
    <p>We used the following packages and facilities to help us train the model:</p>
    <ul>
        <li>OS: Ubuntu 18.04 LTS</li>
        <li>CUDA: 10.01</li>
        <li>GPU: GTX 2080 Ti</li>
        <li>OPENCV: 3.4.4</li>
    </ul>

    <h4>Results</h4>
    <h5>Normal cases:</h5>
    <p>Our result for normal cases was good, and the IOU was 0.973 as following:</p>
    <img class="center" src="COCO_RESULT.png" alt="COCO_RESULT" id="#COCO_RESULT" width="400">


    <h5>Extreme cases:</h5>
    <p>There are some extreme cases when we drive the cars as following pictures. YOLOv3 failed for all of the cases.</p>
    <img class="center" src="EXTREME_CAES.png" alt="EXTREME_CAES" id="#EXTREME_CAES" width="600">

    <p>Extreme cases:</p>
    <p>There are some extreme cases when we driving the cars as following pictures. YOLOv3 failed for all of the cases.</p>
    <img class="center" src="failed_cases.png" alt="failed_cases" id="#failed_cases" width="600">


    <h4>Extreme cases summary:</h4>
    <p>We summarize the extreme cases as following:</p>
    <p>Problem 1. Len Blur</p>
    <img class="center" src="1.png" alt="1" id="#1" width="800">
    <p>Problem 2. Darkening</p>
    <img class="center" src="2.png" alt="2" id="#2" width="800">
    <p>Problem 3. Dirty lens</p>
    <img class="center" src="3.png" alt="3" id="#3" width="800">
    <p>Problem 4. Over-exposure</p>
    <img class="center" src="4.png" alt="4" id="#4" width="800">
    <p>Problem 5. Rain</p>
    <img class="center" src="5.png" alt="5" id="#5" width="800">
    <p>Problem 6. Snow</p>
    <img class="center" src="6.png" alt="6" id="#6" width="800">
    <p>Problem 7. Haze</p>
    <img class="center" src="7.png" alt="7" id="#7" width="800">
    <br>

    <h4>Solution</h4>
    <p>We want to solve the extreme cases problem by reselect data set and image preprossing.</p>
    <p>Better datasets:</p>
    <ol>
        <li>LISA-TS (http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html)</li>
        <ul>
            <li>Pros: North America Standard and cConsover all traffic sign types.</li>
            <li>Cons: The format of annotations does not match our need, lack of essential information and does not have extreme cases.</li>
        </ul>

        <li>CURE-TSR (https://ghassanalregib.com/cure-tsr/)</li>
        <ul>
            <li>Pros: Includes corner cases.</li>
            <li>Cons: Format of annotations.</li>
        </ul>
        <li>Belgium-TS (https://btsd.ethz.ch/shareddata/)</li>

        <li>GTSB (http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)</li>
    </ol>
    <p> We finally chose the LISA-TS as our trianing data set. Following is the peek of LISA-TS:</p>
    <img class="center" src="LISA_PEEK.png" alt="LISA_PEEK" id="#LISA_PEEK" width="600">

    <p>CV pre-processing:</p>
    Our algorithm is as following:
    <img class="center" src="CV.png" alt="CV" id="#CV" width="800">
    
    <h4>YOLO-Tiny + LISA-TS</h4>
    <p>Based on our solutions analyzed above, we choose YOLO-Tiny + LISA-TS as the final model.</p>
    <p>Data-Preparation : We applied a set of tools to help us prepare the data.</p>
    <p>YOLO-Tiny : Instead of 102 layers of YOLOv3, YOLO-Tiny only has 9 layers, which means faster to trian.</p>
    <p>Model Configurations : </p>
    <ul>
        <li>Batches = 64</li>
        <li>Subdivision = 8</li>
        <li>maxbathes = 60000</li>
        <li>steps = 48000, 54000</li>
        <li>Momentum = 0.9</li>
        <li>learning_rate=0.001</li>
        <li>burn_in=1000</li>
        
    </ul>

    <p>Training : </p>
    <ul>
        <li>OS: Ubuntu 18.04 LTS</li>
        <li>CUDA: 10.01</li>
        <li>GPU: GTX 2080 Ti</li>
        <li>OPENCV: 3.4.4</li>  
    </ul>

    <p>Results:</p>
    <img class="center" src="SHOW_1.png" alt="SHOW_1" id="#SHOW_1" width="600">
    <br>
    <img class="center" src="SHOW_2.png" alt="SHOW_2" id="#SHOW_2" width="600">



    <h2>Approach 2 - SVM + HOG</h2>
    <h3>Algorithm</h3>
    <p>Our second approach is called SVM + HOG approach. It includes image processing, ROI (region of interest) selection, HOG feature extraction and supervised learning.</p>
    <p>Our detailed algorithm is shown below.</p>

    <img class="center" src="algo_process.png" alt="algorithm" id="#algorithm" width="900">
    <br>

    <h3>Datasets</h3>
    <h4>Original datasets</h3>
    <p>We randomly downloaded around 135 pictures that contain one or more stop signs from the Internet, to form our original positive dataset. Some of them are shown below.</p>

    <div align="center">
        <img src="orig_pos1.jpg" alt="orig_pos1" height="200">
        <img src="orig_pos2.jpg" alt="orig_pos2" height="200">
        <img src="orig_pos3.jpg" alt="orig_pos3" height="200">
        <img src="orig_pos4.jpg" alt="orig_pos4" height="200">
        <img src="orig_pos5.jpg" alt="orig_pos5" height="200">
        <img src="orig_pos6.jpg" alt="orig_pos6" height="200">
    </div>

    <p>For negative dataset, we download 415 pictures from <a href="#OPTIMOL">[1]</a>. All the pictures do not contain any stop signs. Here are some of them.</p>

    <div align="center">
        <img src="orig_neg1.jpg" alt="orig_neg1" height="200">
        <img src="orig_neg2.jpg" alt="orig_neg2" height="200">
        <img src="orig_neg3.jpg" alt="orig_neg3" height="200">
    </div>

    <h4>Positive training dataset</h4>
    
    <p>The generation of positive training dataset requires a set of image processing steps. Here we use an example <a href="#r1">[2]</a> to illustrate the process how we crop the ROI to generate our positive training dataset.</p>
        

    <p>Step 1, convert the orignial image to HSV color ranges. HSV stands for hue, saturation and value, and is more robust to color segmentation. We define a color range to select out all the red objects, as shown below.</p>
    <img class="center" src="ex1.png" alt="ex1" width="800">
    
    <br>
    
    <p>Step 2, we use a set of morphological transformations, including dialation and erosiion, to romove the noise in the image.</p>
    <img class="center" src="ex2.png" alt="ex2" width="800">

    <br>
    
    <p>Step 3, we fill in holes within each connected component. As you will see, after filling the holes, the text "stop" in the stop sign is removed. In this way, we can remove unneccessary contours and decrease the number of ROI candidates to reduce the running time and hence increase the detection accuracy.</p>
    <img class="center" src="ex3.png" alt="ex3" width="800">
    
    <br>
    
    <p>Step 4, we set a threshold, and discard all the contours with size smaller than that threshold. With this, we can remove small (unneccessary) objects.</p>
    <img class="center" src="ex4.png" alt="ex4" width="800">
    
    <br>
    
    <p>Step 5, for each contour, or connected component, we find its top left and bottom right coordinates. By multiplying these coordinates with some slack factor (here we used 1.1), we can draw the bounding boxes. Each subimage in the bounding boxes is a ROI candidate.</p>
    <img class="center" src="ex5.png" alt="ex5" width="800">
    
    <br>
    
    <p>Step 6, removing all the small bounding boxes that are involved in some larger bounding boxes gives us all the ROI candidates.</p>
    <img class="center" src="ex6.png" alt="ex6" width="800">

    <br>

    <p>Step 7, select out all the subimages within each bounding box and scale them to the same size.</p>
    <img class="center" src="ex7.png" alt="ex7" width="400">

    <br>

    <p>Step 8, remove all the false positives, and keep only the correct ones in our positive training dataset.</p>
    <img class="center" src="ex8.jpg" alt="ex8" width="100">

    <br>

    <p>The image below shows part of our positive training dataset, which represents normal conditions as well as some extreme conditions including raining, snowing, blurring and darkness.</p>
    <img class="center" src="pos_dataset.png" alt="pos_dataset" width="70%">
    <br>

    <h4>Negative training dataset</h4>
    <p>We resized our original negative images (images do not contain stop signs) to some fixed size to form the negative training dataset. Part of the dataset is shown below.</p>
    <img class="center" src="neg_dataset.png" alt="neg_dataset" width="70%">
    <br>

    <h3>Training (HOG + SVM)</h3>
    <p>We extracted HOG features for each image from our positive and negative training dataset. Using the feature of each item as well as their labels, we feed into our weigthed RBF kernel SVM classifier for training.</p>


    <h3>Testing</h3>
    <p>Whenever we get a test image, let's takes the example image we used above for generating the positive training dataset, we can extract ROI candidates following the same steps as we generate positive training dataset. Hence, for the example image, we have the following steps for testing.</p>
    
    <img class="center" src="testing.png" alt="testing" width="800">
    
    <p>That is, given the extracted ROIs, we extract HOG features for each of them. Feed the HOG featues into our trained RBF kernel SVM classifier and test whether it is positive or negative. If positive, we select that ROI and draw a green bouding box around it. Our final result is shown below.</p>
    
    <img class="center" src="testing1.jpg" alt="testing1" width="600">

    <br>

    <h3>Results</h3>
    <p> We tested our model on images taken under extreme weather conditions as well as those with complex backgrounds (images with a lot of confusing red objects that are very similar to stop signs). Our model was very robust to these conditions and can detect the stop signs in these images accurately.</p>
    
    
    <img class="center" src="results1.png" alt="results1" width="600">
    <br>
    <img class="center" src="results2.png" alt="results2" width="600">

    <br>

    <p>Furthermore, we drove in the city of Madison, WI in both rainy and sunny days and took the videos. Our model again showed robust ability of detecting and tracking the stop signs under different scenarios. Videos are shown below.</p>

    <div align="center">
        <iframe src="https://www.youtube.com/embed/L8ZecvQdd00?autoplay=0"></iframe>
        <iframe src="https://www.youtube.com/embed/EmXHeHNq6tU?autoplay=0"></iframe>
    </div>

    <br>

    <h3>Discussion</h3>

    <h4>Running time</h4>

    <p>This SVM + HOG approach is relatively fast compared to other models. For example, the one minute video takes about two minutes to process, and we can see the very small stop signs can even be detected.</p>

    <p>Acutally, the running time largely depends on how much accuracy we want to achieve. If we make the threshold larger, more small objects will be filtered out and we can only detect the stop signs that are large enough (in pixels). However, if we want to detect the stop signs faraway, we have to make the threshold smaller and that will take more time to process.</p>

    <h4>Pros</h4>
    <p>Here are some of the advantages of our HOG + SVM model:</p>
    <ol>
        <li>The running time is good compared to other models, especially to deep learning models.</li>
        <li>It can achieve high accuracy if the threshold is small enough.</li>
        <li>It can detect robustly under most extreme weather conditions, especially in cloudy, rainy and snowy weather.</li>
        <li>It is easy to implement and it does not need expensive hardware.</li>
    </ol>
    
    <h4>Cons</h4>
    <p>Although the results (images and videos) show that our model is very robust to some of the extreme conditions, there are still certain cases where this model cannot work very well. These include:</p>
        <ul>
            <li>The weather is extremely dark, then our color mask will just filter out the stop signs.</li>
            <li>When there are red objects covering or connecting to part of the stop sign, the image processing process will treat them as a single object, and hence the supervised learning classifier will vote for "no".</li>
        </ul>
    </p>
    <br>



    <h2>References</h2>
    <ol>
        <li id="OPTIMOL">http://vision.stanford.edu/projects/OPTIMOL/category/stop-sign/catmain.html</li>
        <li id="r1">https://www.brooklynpaper.com/assets/photos/40/14/dtg-dumbo-stop-sign-safest-ever-2017-04-07-bk01_z.jpg</li>
    </ol>
    
    

</div>


<footer>
    <p></p>
</footer>


</body>
</html>
